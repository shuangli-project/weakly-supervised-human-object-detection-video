<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en"><head>
  <title>Weakly Supervised Human and Object Detection via Spatiotemporal Interactions</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="fig/comp_cartoon.jpg"/>
<meta property="og:title" content="Energy-Based Models for Continual Learning" />

<script src="lib.js" type="text/javascript"></script>
<script src="popup.js" type="text/javascript"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=1000,height=1000,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
    PADDING-RIGHT: 0px;
    PADDING-LEFT: 0px;
    FLOAT: right;
    PADDING-BOTTOM: 0px;
    PADDING-TOP: 0px
}
#primarycontent {
    MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
BODY {
    TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="b5m.js" id="b5mmain" type="text/javascript"></script></head>

<body>

<div id="primarycontent">
<center><h1>Weakly Supervised Human and Object Detection via Spatiotemporal Interactions</h1></center>
<center><h2>
  <a href="http://www.mit.edu/~lishuang/">Shuang Li<sup>1</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="https://yilundu.github.io/">Yilun Du<sup>1</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba<sup>1</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="https://www.di.ens.fr/~josef/">Josef Sivic<sup>2</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="https://research.adobe.com/person/bryan-russell/">Bryan Russell<sup>3</sup></a>
</h2></center>
<center><h2>
  <sup>1</sup> MIT CSAIL&nbsp;&nbsp;&nbsp;
  <sup>2</sup> INRIA&nbsp;&nbsp;&nbsp;
  <sup>3</sup> Adobe
</h2></center>
<center><h2><strong><a href="https://arxiv.org/pdf/2011.12216.pdf">Paper</a> | <a href="https://github.com/ShuangLI59/ebm-continual-learning">PyTorch code</a></strong> </h2></center>
<br>


<!---
<center><a>
<img src="fig/comp_cartoon.jpg" width="1000"> 
</a></center>
<br>
-->



<p>
<h2><b>Abstract</b></h2>

<div style="font-size:14px"><p align="justify">
  We introduce the task of weakly supervised learning for detecting a human and object in videos given a query action and object. Our task poses unique challenges as a system does not know whether the human-object interaction is present in the untrimmed video or the actual spatiotemporal location of the human and object. To address these challenges, we introduce a weakly supervised spatiotemporal training loss that aims to jointly associate spatiotemporal regions in a video with an action and object vocabulary and encourage temporal continuity of the visual appearance of moving objects. To train our model, we introduce a dataset comprising over 7k videos with human-object interaction annotations that have been semi-automatically curated from sentence captions associated with the videos. We demonstrate improved performance over weakly supervised baselines adapted to our task on our video dataset. 
</div>
</p>




<!-- ---------------------------------------------------------------------------------- -->
<a href=""><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="files/fig/paper_thumbnail.png" width=180></a>
<br>


<h2>Paper:
<a href="https://arxiv.org/pdf/2011.12216.pdf">arxiv, 2021 </a>
</h2>

<h2>Dataset: 
<a href='https://shuangli-project.github.io/VHICO-Dataset/'>V-HICO</a>
</h2>

<h2>Code: 
<a href='https://github.com/ShuangLI59/weakly-supervised-human-object-detection-video'>PyTorch</a>
</h2>


<h2>Citation</h2>
<p>Shuang Li, Yilun Du, Antonio Torralba, Josef Sivic, Bryan Russell. <br>
   Weakly Supervised Human and Object Detection via Spatiotemporal Interactions. <a href="files/citation.txt">Bibtex</a>
</p>


<br><br><br>


<!-- ---------------------------------------------------------------------------------- -->
<h2><b>Human and Object Detection in Videos</b></h2>
We seek to detect human-object interactions in videos. In this example, our system is able to find ``human grooming horse'' in a dataset of videos. Our approach learns to detect such interactions in a weakly supervised fashion. 
<br>
<center><a> <img src="files/fig/teaser.png" style="float:none;width:500px"> </a></center>
<br>

<h2><b>Weakly supervised spatiotemporal loss</b></h2>
Our loss jointly aligns features for spatiotemporal regions in a video to (a) a language-embedding feature for an input query and (b) other spatiotemporal regions likely to contain the object.
<br>
<center><a> <img src="files/fig/loss.png" style="float:none;width:900px"> </a></center>
<br>

<h2><b>Model of Spatiotemporal Interactions</b></h2>
Model architecture of the proposed method. (a) is the overview of the training process. We take video frames as input and send them to a temporal soft attention module and a feature aggregation module. The output features incorporate the temporal and spatial information and are used to compute the spatiotemporal loss. 
(b) is the illustration of the feature aggregation module.
In each frame, we use the Faster-RCNN/Densepose to extract the object and human region proposal locations and an ROI pooling layer to extract their features. The object/human score functions take the feature of a region proposal from the ROI pooling layer, the language phrase, and the frame feature $x'_t$ (do not show here) as input and output a score for the region. We aggregate the object/human region proposal features based on their scores from the score function to generate the aggregated object/human features.
<br>
<center><a> <img src="files/fig/model_pipeline.png" style="float:none;width:500px"> </a></center>
<br>


<!-- ---------------------------------------------------------------------------------- -->
<h2><b>Experiments</b></h2>
<h3>Ablation Studies on V-HICO</h3>
Evaluation of each component of the proposed model.  Phrase (Phr) detection refers to correct localization (0.5 IoU) of the union of human and object bounding boxes while relationship (Rel) refers to correct localization of both human and object bounding boxes. 
<br>
<br>
<center><a> <img src="files/fig/table1.png" style="float:none;width:800px"> </a></center>
<br>

<h3>Comparison with Baselines</h3>
Evaluation of performance on HICO-DET compared to methods in [1], [2], and [3] and different random baselines. Phrase detection refers to correct localization (0.5 IoU) of the union of human and object bounding boxes while relationship refers to correct localization of both human and object bounding boxes. 
<br>
<br>
<center><a> <img src="files/fig/table1.png" style="float:none;width:800px"> </a></center>
<br>


<h3>Comparison with Baselines on Unseen Classes</h3>
Evaluation of various models on the unseen test set on V-HICO. The unseen test set consists of 53 classes of objects unseen during training. Evaluation at IoU threshold 0.5. (ko) and (def) are the known object setting and default setting.
<br>
<br>
<center><a> <img src="files/fig/table1.png" style="float:none;width:800px"> </a></center>
<br>


<h3>Qualitative Results</h3>
Qualitative prediction of our model on the V-HICO test set with predicted human bounding box (red), predicted object bounding box (green), and input action-object label (bottom).
<br>
<br>
<center><a> <img src="files/fig/results.png" style="float:none;width:900px"> </a></center>
<br>





<!-- ---------------------------------------------------------------------------------- -->
<h2><b>Related works</b></h2>

<ol id='relatedwork' type="1">
<li font-size: 15px>
 <a href="https://arxiv.org/abs/1707.09472"><strong>"Weakly-supervised learning of visual relations"</strong></a>, in ICCV 2017 (Oral).
</li>
<li font-size: 15px>
 Luowei Zhou, Nathan Louis, Jason J. Corso <a href="https://arxiv.org/abs/1805.02834"><strong>"Weakly-supervised video object grounding from text by loss weighting and object interaction"</strong></a>, in BMVC 2018.
</li>
<li font-size: 15px>
 <a href="https://arxiv.org/abs/2007.08814"><strong>"Visual Relation Grounding in Videos"</strong></a>, in ECCV 2020.
</li>
</ol>


<br><br><br><br>
<center>
  <a href="http://accessibility.mit.edu/">Accessibility</a>
</center>
<br>


<!-- ---------------------------------------------------------------------------------- -->
<div style="display:none">
<script type="text/javascript" src="http://gostats.com/js/counter.js"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script>
<noscript><a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"><img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" /></a></noscript>
</div>
</body></html
>

