<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Energy-Based Models for Continual Learning">
    <meta name="author"
          content="Shuang Li, Yilun Du, Antonio Torralba, Josef Sivic, Bryan Russell">

    <title>Weakly Supervised Human-Object Interaction Detection in Video via Contrastive Spatiotemporal Regions</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Weakly Supervised Human-Object Interaction Detection in Video <br> via Contrastive Spatiotemporal Regions</h2>
    <!-- <h3>ICML 2021</h3> -->
    <hr>
    <p class="authors">
        <a href="https://people.csail.mit.edu/lishuang/"> Shuang Li<sup>1</sup></a>,
        <a href="https://yilundu.github.io">Yilun Du<sup>1</sup></a>,
        <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba<sup>1</sup></a>,
        <a href="https://www.di.ens.fr/~josef/">Josef Sivic<sup>2</sup></a>,
        <a href="https://research.adobe.com/person/bryan-russell/">Bryan Russell<sup>3</sup></a>
    </p>

    <p class="institution">
      <sup>1</sup> MIT CSAIL&nbsp;&nbsp;&nbsp;
      <sup>2</sup> CIIRC CTU&nbsp;&nbsp;&nbsp;
      <sup>3</sup> Adobe
    </p>


    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="">Paper</a>
        <a class="btn btn-primary" href="https://shuangli-project.github.io/VHICO-Dataset/">Dataset</a>
        <a class="btn btn-primary" href="https://github.com/ShuangLI59/weakly-supervised-human-object-detection-video">Code</a>
    </div>
</div>


<div class="container">
    <div class="section">
        <p>
            We introduce the task of weakly supervised learning for detecting human and object interactions in videos. Our task poses unique challenges as a system does not know what types of human-object interactions are present in a video or the actual spatiotemporal location of the human and object. To address these challenges, we introduce a contrastive weakly supervised training loss that aims to jointly associate spatiotemporal regions in a video with an action and object vocabulary and encourage temporal continuity of the visual appearance of moving objects as a form of self-supervision. To train our model, we introduce a dataset comprising over 6.5k videos with human-object interaction annotations that have been semi-automatically curated from sentence captions associated with the videos. We demonstrate improved performance over weakly supervised baselines adapted to our task on our video dataset. 
        </p>
        <br>
        <div class="row justify-content-center">
            <div class="col-sm-2">
            </div>
            <div class="col-sm-6">
                <img src="files/fig/teaser7.png" style="width:100%">
            </div>
            <div class="col-sm-2">
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Training Overview</h2>
        <hr>
        <p>
            Given a video clip and a verb-object query, for each frame, we first extract its human and object region features. The human/object features are aggregated after a region attention module. The attended human feature, attended object feature, the feature of verb-object query, and object region features from other frames are used to compute our weakly supervised contrastive loss.
		</p>

        <div class="row justify-content-center">
            <div class="col-sm-2">
            </div>
            <div class="col-sm-8">
                <img src="files/fig/training_overview4.png" style="width:100%">
            </div>
            <div class="col-sm-2">
            </div>
        </div>

    </div>


    <div class="section">
        <h2>Weakly supervised contrastive loss</h2>
        <hr>
        <p>
            Our loss jointly aligns features for spatiotemporal regions in a video to (a) a language-embedding feature for an input verb-object query and (b) other spatiotemporal regions likely to contain the target object. This figure only shows object regions. The same thing is applied to human regions.
        </p>

        <div class="row justify-content-center">
            <div class="col-sm-1">
            </div>
            <div class="col-sm-12">
                <img src="files/fig/loss4.png" style="width:100%">
            </div>
            <div class="col-sm-1">
            </div>
        </div>

    </div>


<!-- 
    <div class="section">
        <h2>Ablation studies on V-HICO</h2>
        <hr>
        <p>
            Evaluation of each component of the proposed model. Phrase (Phr) detection refers to correct localization (0.3 IoU) of the union of human and object bounding boxes while relation (Rel) refers to correct localization (0.3 IoU) of both human and object bounding boxes.
        </p>

        <div class="row justify-content-center">
            <div class="col-sm-1">
            </div>
            <div class="col-sm-12">
                <img src="files/fig/res1.png" style="width:100%">
            </div>
            <div class="col-sm-1">
            </div>
        </div>

    </div> -->

    <div class="section">
        <h2>Comparison with baselines</h2>
        <hr>
        <p>
            Evaluation of performance on the V-HICO dataset. Phrase (Phr) detection refers to correct localization (0.3 IoU) of the union of human and object bounding boxes while relation (Rel) refers to correct localization (0.3 IoU) of both human and object bounding boxes. (ko) and (def) are the known object setting and default setting.
        </p>

        <div class="row justify-content-center">
            <div class="col-sm-1">
            </div>
            <div class="col-sm-10">
                <img src="files/fig/res2.png" style="width:100%">
            </div>
            <div class="col-sm-1">
            </div>
        </div>

    </div>


    <div class="section">
        <h2>Comparison with baselines on unseen classes</h2>
        <hr>
        <p>
            Evaluation of our proposed approach and baselines on the unseen test set on V-HICO. The unseen test set consists of 51 classes of objects unseen during training. Evaluation at IoU threshold 0.3. 
        </p>

        <div class="row justify-content-center">
            <div class="col-sm-3">
            </div>
            <div class="col-sm-10">
                <img src="files/fig/res3.png" style="width:100%">
            </div>
            <div class="col-sm-3">
            </div>
        </div>

    </div>


    <!-- <div class="section">
        <h2>Qualitative results</h2>
        <hr>
        <p>
            Qualitative predictions of our model with top predicted human bounding boxes (yellow) and object bounding boxes (blue).
        </p>

        <div class="row justify-content-center">
            <div class="col-sm-3">
            </div>
            <div class="col-sm-12">
                <img src="files/fig/result.png" style="width:100%">
            </div>
            <div class="col-sm-3">
            </div>
        </div>

    </div>



    <div class="section">
        <h2>Failure  cases</h2>
        <hr>
        <p>
            We show the predicted human (yellow) and object (blue) bounding boxes. Left: human prediction is wrong. Middle: blurry object. Right: challenging scene.
        </p>

        <div class="row justify-content-center">
            <div class="col-sm-2">
            </div>
            <div class="col-sm-8">
                <img src="files/fig/failure.png" style="width:100%">
            </div>
            <div class="col-sm-2">
            </div>
        </div>

    </div> -->


    <div class="section">
        <div class="section">
            <h2>Paper</h2>
            <hr>
            <div>
                <div class="list-group">
                    <a href=""
                       class="list-group-item">
                        <img src="files/fig/paper_thumbnail.png"
                             style="width:100%; margin-right:-20px; margin-top:-10px;">
                    </a>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Bibtex</h2>
            <hr>
            <div class="bibtexsection">
                @article{li2020energy,
                  title={Energy-Based Models for Continual Learning},
                  author={Li, Shuang and Du, Yilun and van de Ven, Gido M and Mordatch, Igor},
                  journal={arXiv preprint arXiv:2011.12216},
                  year={2020}
                }
            </div>
        </div>

        <hr>

        <footer>
            <p>Send feedback and questions to <a href="https://people.csail.mit.edu/lishuang/">Shuang Li</a></p>
        </footer>
    </div>

</body>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>
